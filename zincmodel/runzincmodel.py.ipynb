{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4138dc9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import csv \n",
    "\n",
    "from preprocessingfun import preprocessingzinc, sigmastd, createincompletedata, dataindices\n",
    "from miscfun import round_to_poweroften, calculatemse, calculatemaxerror\n",
    "from othermodels import gaussianmodel, Ridgemodel, MLPmodel\n",
    "from mainmodel import truncatednormalmodel, samplefromlikelihood\n",
    "from coveragefun import calculatecoverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f376085",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataflows = pd.read_csv(\"data/zincflowschina.txt\")\n",
    "datastocks = pd.read_csv(\"data/zincstockchina.txt\")\n",
    "\n",
    "#preprocess the data\n",
    "datamat,outputdatachildprocess,availablechildstocksandflows,m,N=preprocessingzinc(datastocks,dataflows)\n",
    "\n",
    "priorcov=16.0\n",
    "priorcovvague=40.0\n",
    "alpha=sigmastd/priorcovvague\n",
    "\n",
    "#construct the prior, uninformative\n",
    "\n",
    "priormeanvague = np.array([1.0] * N)\n",
    "covariancevecvague = 40.0  * np.ones(N)\n",
    "\n",
    "dataavg=np.mean(np.abs(datamat[:,1]))\n",
    "\n",
    "for row in datamat:\n",
    "    priormeanvague[int(row[0])]=np.sign(row[1])*dataavg\n",
    "    covariancevecvague[int(row[0])] = priorcovvague\n",
    "\n",
    "priormeanvaguecompact = priormeanvague[availablechildstocksandflows]\n",
    "covariancevecvaguecompact = covariancevecvague[availablechildstocksandflows]\n",
    "priorcovariancevaguecompact = np.diag(covariancevecvaguecompact)\n",
    "\n",
    "#construct the weakly informative prior\n",
    "\n",
    "priormean = np.array([1.0] * N)\n",
    "covariancevec = 16.0  * np.ones(N)\n",
    "\n",
    "for row in datamat:\n",
    "    priormean[int(row[0])]=round_to_poweroften(row[1])\n",
    "    covariancevec[int(row[0])]=max(min(priorcov,16*round_to_poweroften(row[1])**2),0.01)\n",
    "\n",
    "priormeancompact = priormean[availablechildstocksandflows]\n",
    "covarianceveccompact = covariancevec[availablechildstocksandflows]\n",
    "priorcovariancecompact = np.diag(covarianceveccompact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbd1a59",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Calculate the root mean squared error and maximum error of a number of models, specifically\n",
    "#Proposed model, Gaussian model, Ridge regression, MLP over for each dataset size from 0 to 20,\n",
    "#averaged over 50 runs.\n",
    "\n",
    "truevalues=outputdatachildprocess['quantity'][availablechildstocksandflows]\n",
    "for seed in range(0,50):\n",
    "    random.seed(seed)\n",
    "    pointsall = random.sample(list(availablechildstocksandflows), len(availablechildstocksandflows))\n",
    "    print('seed')\n",
    "    print(seed)\n",
    "    for i in range(0,21): \n",
    "        points=pointsall[0:i]\n",
    "        print('i')\n",
    "        print(i)\n",
    "        incompletedesigncompact,incompletematrixstockscompact,incompletematrixflowscompact,incompletedatavector,incompletedataflownumber=createincompletedata(points,datamat,availablechildstocksandflows,m,N) \n",
    "\n",
    "        regcoef=Ridgemodel(incompletedesigncompact,incompletedatavector,alpha)\n",
    "        mseridge=calculatemse(regcoef,truevalues)\n",
    "        maxerrorridge=calculatemaxerror(regcoef,truevalues)\n",
    "        \n",
    "        resultsridge=[i,mseridge,maxerrorridge,seed,alpha]\n",
    "\n",
    "        import csv\n",
    "        with open(\"zincridgeerror.txt\", \"a\") as f0:\n",
    "            wr0 = csv.writer(f0)\n",
    "            wr0.writerow(resultsridge)\n",
    "        f0.close()\n",
    "\n",
    "        posteriormeangauss,posteriorcovariancegauss=gaussianmodel(priormeancompact,priorcovariancecompact,incompletedesigncompact,incompletedatavector)\n",
    "        posteriormeangaussvague,posteriorcovariancegaussvague=gaussianmodel(priormeanvaguecompact,priorcovariancevaguecompact,incompletedesigncompact,incompletedatavector)\n",
    "        \n",
    "        \n",
    "        msegauss=calculatemse(posteriormeangauss,truevalues)\n",
    "        maxerrorgauss=calculatemaxerror(posteriormeangauss,truevalues)\n",
    "        \n",
    "        msegaussvague=calculatemse(posteriormeangaussvague,truevalues)\n",
    "        maxerrorgaussvague=calculatemaxerror(posteriormeangaussvague,truevalues)\n",
    "        \n",
    "        resultsgauss=[i,msegauss,maxerrorgauss,seed,sigmastd,covariancevec[int(row[0])]]\n",
    "        resultsgaussvague=[i,msegaussvague,maxerrorgaussvague,seed,sigmastd,covariancevecvague[int(row[0])]]\n",
    "\n",
    "        import csv\n",
    "        with open(\"zincweaklyinformativegaussianerror.txt\", \"a\") as f1:\n",
    "            wr1 = csv.writer(f1)\n",
    "            wr1.writerow(resultsgauss)\n",
    "        f1.close()\n",
    "        with open(\"zincuninformativegaussianerror.txt\", \"a\") as f2:\n",
    "            wr2 = csv.writer(f2)\n",
    "            wr2.writerow(resultsgaussvague)\n",
    "        f2.close()\n",
    "\n",
    "        modelpred,traceoutput=truncatednormalmodel(priormean,covariancevec,incompletematrixstockscompact,incompletematrixflowscompact,incompletedatavector,incompletedataflownumber,availablechildstocksandflows,m,1)\n",
    "        \n",
    "        truncatednormalmse=calculatemse(modelpred,truevalues)\n",
    "        truncatednormalmaxerror=calculatemaxerror(modelpred,truevalues)\n",
    "        \n",
    "        resultstruncatednormal=[i,truncatednormalmse,truncatednormalmaxerror,seed,sigmastd,covariancevec[int(row[0])]]\n",
    "\n",
    "        with open(\"zincweaklyinformativemodelerror.txt\", \"a\") as f3:\n",
    "            wr3 = csv.writer(f3)\n",
    "            wr3.writerow(resultstruncatednormal)\n",
    "        f3.close()\n",
    "\n",
    "        modelpredvague,traceoutputvague=truncatednormalmodel(priormeanvague,covariancevecvague,incompletematrixstockscompact,incompletematrixflowscompact,incompletedatavector,incompletedataflownumber,availablechildstocksandflows,m,1)\n",
    "        \n",
    "        truncatednormalmsevague=calculatemse(modelpredvague,truevalues)\n",
    "        truncatednormalmaxerrorvague=calculatemaxerror(modelpredvague,truevalues)\n",
    "        \n",
    "        resultstruncatednormalvague=[i,truncatednormalmsevague,truncatednormalmaxerrorvague,seed,sigmastd,covariancevecvague[int(row[0])]]\n",
    "\n",
    "        with open(\"zincuninformativemodelerror.txt\", \"a\") as f4:\n",
    "            wr4 = csv.writer(f4)\n",
    "            wr4.writerow(resultstruncatednormalvague)\n",
    "        f4.close()\n",
    "        \n",
    "        \n",
    "        mlppred=MLPmodel(incompletedesigncompact,incompletedatavector)\n",
    "        \n",
    "        msemlp=calculatemse(mlppred,truevalues)\n",
    "        maxerrormlp=calculatemaxerror(mlppred,truevalues)\n",
    "\n",
    "        resultsmlp=[i,msemlp,maxerrormlp,seed]\n",
    "\n",
    "        import csv\n",
    "        with open(\"zincmlperror.txt\", \"a\") as f5:\n",
    "            wr5 = csv.writer(f5)\n",
    "            wr5.writerow(resultsmlp)\n",
    "        f5.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f34933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#generate rmse plots of the four models, based on model outputs from previous cell\n",
    "\n",
    "zincgaussianmse = np.loadtxt(\"zincweaklyinformativegaussianerror.txt\", comments=\"#\", delimiter=\",\", unpack=False)\n",
    "zincgaussianmsevague = np.loadtxt(\"zincuninformativegaussianerror.txt\", comments=\"#\", delimiter=\",\", unpack=False)\n",
    "\n",
    "zincmapmse = np.loadtxt(\"zincweaklyinformativemodelerror.txt\", comments=\"#\", delimiter=\",\", unpack=False)\n",
    "zincmapmsevague = np.loadtxt(\"zincuninformativemodelerror.txt\", comments=\"#\", delimiter=\",\", unpack=False)\n",
    "\n",
    "zincfreqmse = np.loadtxt(\"zincridgeerror.txt\", comments=\"#\", delimiter=\",\", unpack=False)\n",
    "\n",
    "zincmlpmse=np.loadtxt(\"zincmlperror.txt\", comments=\"#\", delimiter=\",\", unpack=False)\n",
    "\n",
    "zincgaussianmsedf = pd.DataFrame(dict(n=zincgaussianmse[:,0], l2error=zincgaussianmse[:,1]))\n",
    "\n",
    "zincgaussianmsevaguedf = pd.DataFrame(dict(n=zincgaussianmsevague[:,0], l2error=zincgaussianmsevague[:,1]))\n",
    "\n",
    "zincmapmsedf = pd.DataFrame(dict(n=zincmapmse[:,0], l2error=zincmapmse[:,1]))\n",
    "\n",
    "zincmapmsevaguedf = pd.DataFrame(dict(n=zincmapmsevague[:,0], l2error=zincmapmsevague[:,1]))\n",
    "\n",
    "zincfreqmsedf = pd.DataFrame(dict(n=zincfreqmse[:,0], l2error=zincfreqmse[:,1]))\n",
    "\n",
    "zincmlpmsedf = pd.DataFrame(dict(n=zincmlpmse[:,0], l2error=zincmlpmse[:,1]))\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,})\n",
    "\n",
    "zincgaussianmsedfmean=zincgaussianmsedf.groupby(['n'],as_index=False).mean()\n",
    "zincgaussianmsevaguedfmean=zincgaussianmsevaguedf.groupby(['n'],as_index=False).mean()\n",
    "zincmapmsedfmean=zincmapmsedf.groupby(['n'],as_index=False).mean()\n",
    "zincmapmsevaguedfmean=zincmapmsevaguedf.groupby(['n'],as_index=False).mean()\n",
    "\n",
    "zincfreqmsedfmean=zincfreqmsedf.groupby(['n'],as_index=False).mean()\n",
    "\n",
    "zincmlpmsedfmean=zincmlpmsedf.groupby(['n'],as_index=False).mean()\n",
    "\n",
    "plt.xticks(range(0,21)) #ensures integer values on x axis. \n",
    "\n",
    "plt.plot(zincgaussianmsedfmean['n'], zincgaussianmsedfmean['l2error'],label='Informative Prior, Gaussian Model')\n",
    "plt.plot(zincgaussianmsevaguedfmean['n'],zincgaussianmsevaguedfmean['l2error'],label='Vague Prior, Gaussian Model')\n",
    "\n",
    "plt.plot(zincmapmsedfmean['n'], zincmapmsedfmean['l2error'],label='Informative Prior, Proposed Model')\n",
    "plt.plot(zincmapmsevaguedfmean['n'], zincmapmsevaguedfmean['l2error'],label='Vague Prior, Proposed Model')\n",
    "plt.plot(zincfreqmsedfmean['n'], zincfreqmsedfmean['l2error'],label='Ridge Regression')\n",
    "\n",
    "plt.plot(zincmlpmsedfmean['n'], zincmlpmsedfmean['l2error'],label='Multilayer Perceptron')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('$\\displaystyle n$',fontsize=17)\n",
    "plt.xlabel('Number of data points',fontsize=17)\n",
    "plt.ylabel('RMSE',fontsize=17)\n",
    "plt.savefig('Overallmse.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be75a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate maximum error plots of the four models\n",
    "\n",
    "zincgaussianmaxerror = np.loadtxt(\"zincweaklyinformativegaussianerror.txt\", comments=\"#\", delimiter=\",\", unpack=False)\n",
    "zincgaussianmaxerrorvague = np.loadtxt(\"zincuninformativegaussianerror.txt\", comments=\"#\", delimiter=\",\", unpack=False)\n",
    "\n",
    "zincmapmaxerror = np.loadtxt(\"zincweaklyinformativemodelerror.txt\", comments=\"#\", delimiter=\",\", unpack=False)\n",
    "zincmapmaxerrorvague = np.loadtxt(\"zincuninformativemodelerror.txt\", comments=\"#\", delimiter=\",\", unpack=False)\n",
    "\n",
    "zincfreqmaxerror = np.loadtxt(\"zincridgeerror.txt\", comments=\"#\", delimiter=\",\", unpack=False)\n",
    "\n",
    "zincmlpmaxerror=np.loadtxt(\"zincmlperror.txt\", comments=\"#\", delimiter=\",\", unpack=False)\n",
    "\n",
    "zincgaussianmaxerrordf = pd.DataFrame(dict(n=zincgaussianmaxerror[:,0], maxerror=zincgaussianmaxerror[:,2]))\n",
    "\n",
    "zincgaussianmaxerrorvaguedf = pd.DataFrame(dict(n=zincgaussianmaxerrorvague[:,0], maxerror=zincgaussianmaxerrorvague[:,2]))\n",
    "\n",
    "zincmapmaxerrordf = pd.DataFrame(dict(n=zincmapmaxerror[:,0], maxerror=zincmapmaxerror[:,2]))\n",
    "\n",
    "zincmapmaxerrorvaguedf = pd.DataFrame(dict(n=zincmapmaxerrorvague[:,0], maxerror=zincmapmaxerrorvague[:,2]))\n",
    "\n",
    "zincfreqmaxerrordf = pd.DataFrame(dict(n=zincfreqmaxerror[:,0], maxerror=zincfreqmaxerror[:,2]))\n",
    "\n",
    "zincmlpmaxerrordf = pd.DataFrame(dict(n=zincmlpmaxerror[:,0], maxerror=zincmlpmaxerror[:,2]))\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,})\n",
    "\n",
    "zincgaussianmaxerrordfmean=zincgaussianmaxerrordf.groupby(['n'],as_index=False).mean()\n",
    "zincgaussianmaxerrorvaguedfmean=zincgaussianmaxerrorvaguedf.groupby(['n'],as_index=False).mean()\n",
    "zincmapmaxerrordfmean=zincmapmaxerrordf.groupby(['n'],as_index=False).mean()\n",
    "zincmapmaxerrorvaguedfmean=zincmapmaxerrorvaguedf.groupby(['n'],as_index=False).mean()\n",
    "\n",
    "zincfreqmaxerrordfmean=zincfreqmaxerrordf.groupby(['n'],as_index=False).mean()\n",
    "\n",
    "zincmlpmaxerrordfmean=zincmlpmaxerrordf.groupby(['n'],as_index=False).mean()\n",
    "\n",
    "plt.xticks(range(0,21))\n",
    "\n",
    "plt.plot(zincgaussianmaxerrordfmean['n'], zincgaussianmaxerrordfmean['maxerror'],label='Informative Prior, Gaussian Model')\n",
    "plt.plot(zincgaussianmaxerrorvaguedfmean['n'],zincgaussianmaxerrorvaguedfmean['maxerror'],label='Vague Prior, Gaussian Model')\n",
    "\n",
    "plt.plot(zincmapmaxerrordfmean['n'], zincmapmaxerrordfmean['maxerror'],label='Informative Prior, Proposed Model')\n",
    "plt.plot(zincmapmaxerrorvaguedfmean['n'], zincmapmaxerrorvaguedfmean['maxerror'],label='Vague Prior, Proposed Model')\n",
    "plt.plot(zincfreqmaxerrordfmean['n'], zincfreqmaxerrordfmean['maxerror'],label='Ridge Regression')\n",
    "\n",
    "plt.plot(zincmlpmaxerrordfmean['n'], zincmlpmaxerrordfmean['maxerror'],label='Multilayer Perceptron')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('$\\displaystyle n$',fontsize=17)\n",
    "plt.xlabel('Number of data points',fontsize=17)\n",
    "plt.ylabel('Max error',fontsize=17)\n",
    "plt.savefig('Overallmaxerror.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8db5c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for calculating frequentist coverage of marginal posterior 95% HDI\n",
    "\n",
    "pointsall=[2, 14, 4, 32, 59, 67, 48, 0, 9, 36, 18, 7, 15, 19, 27, 33, 6, 5, 57, 12]\n",
    "truevalues=outputdatachildprocess['quantity'][availablechildstocksandflows]\n",
    "\n",
    "i=10 #size of subset of data used, we used i=5,10,15,20 for results in the paper\n",
    "\n",
    "points=pointsall[0:i]\n",
    "incompletedesigncompact,incompletematrixstockscompact,incompletematrixflowscompact,incompletedatavector,incompletedataflownumber=createincompletedata(points,datamat,availablechildstocksandflows,m,N)\n",
    "\n",
    "numberofdata=300\n",
    "#generate 300 datasets from the likelihood\n",
    "sampledatavector=samplefromlikelihood(incompletedatavector, incompletedataflownumber, numberofdata, m)\n",
    "\n",
    "np.savetxt('sampledatavector'+str(i)+'data'+str(numberofdata)+'times.txt', sampledatavector,delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238c1ff6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#For each dataset, calculate the posterior 95% HDI and check the coverage, weakly informative prior\n",
    "\n",
    "for times in range(0,numberofdata): \n",
    "    print('times')\n",
    "    print(times)\n",
    "    modelpred,trace=truncatednormalmodel(priormean,covariancevec,incompletematrixstockscompact,incompletematrixflowscompact, \\\n",
    "                                        sampledatavector[times,:],incompletedataflownumber,availablechildstocksandflows,m,0)\n",
    "\n",
    "    cr_95_quantile,distancetotruth,ci_95_marginal=calculatecoverage(modelpred, trace, availablechildstocksandflows, outputdatachildprocess)\n",
    "\n",
    "    checkcoveragetruncatednormal=[cr_95_quantile,distancetotruth,distancetotruth<cr_95_quantile,times]\n",
    "\n",
    "    with open(\"checkcoverageweaklyinformativeprior\"+str(i)+\".txt\", \"a\") as truncatednormalcoveragefile:\n",
    "        wr = csv.writer(truncatednormalcoveragefile)\n",
    "        wr.writerow(checkcoveragetruncatednormal)\n",
    "        truncatednormalcoveragefile.close()\n",
    "\n",
    "    with open(\"checkcoveragemarginalweaklyinformativeprior\"+str(i)+\".txt\", \"a\") as truncatednormalmarginalcoveragefile:\n",
    "        np.savetxt(truncatednormalmarginalcoveragefile,ci_95_marginal, delimiter=',', fmt='%s')\n",
    "        truncatednormalmarginalcoveragefile.close()\n",
    "        \n",
    "    with open(\"sampleddata\"+str(i)+\".txt\", \"a\") as sampleddatafile:\n",
    "            wr = csv.writer(sampleddatafile)\n",
    "            wr.writerow(np.array([pointsall[0:i],sampledatavector[times,:]]))\n",
    "            #np.savetxt(sampleddatafile,ci_95_marginal, delimiter=',', fmt='%s')\n",
    "            sampleddatafile.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85682c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each dataset, calculate the posterior 95% HDI and check the coverage, uninformative prior\n",
    "\n",
    "for times in range(0,numberofdata):\n",
    "    print('times')\n",
    "    print(times)\n",
    "    modelpredvague,tracevague = truncatednormalmodel(priormeanvague,covariancevecvague, incompletematrixstockscompact, \\\n",
    "        incompletematrixflowscompact,sampledatavector[times, :], incompletedataflownumber, availablechildstocksandflows,m,0)\n",
    "    \n",
    "    cr_95_quantilevague,distancetotruthvague,ci_95_marginalvague=calculatecoverage(modelpredvague, tracevague, availablechildstocksandflows, outputdatachildprocess)\n",
    "    \n",
    "    checkcoveragetruncatednormalvague = [cr_95_quantilevague, distancetotruthvague, distancetotruthvague < cr_95_quantilevague, times]\n",
    "\n",
    "    with open(\"checkcoverageuninformativeprior\" + str(i) +\".txt\", \"a\") as truncatednormalcoveragefilevague:\n",
    "        wr = csv.writer(truncatednormalcoveragefilevague)\n",
    "        wr.writerow(checkcoveragetruncatednormalvague)\n",
    "        truncatednormalcoveragefilevague.close()\n",
    "\n",
    "    with open(\"checkcoveragemarginaluninformativeprior\" + str(i) +\".txt\",\n",
    "              \"a\") as truncatednormalmarginalcoveragefilevague:\n",
    "        np.savetxt(truncatednormalmarginalcoveragefilevague, ci_95_marginalvague, delimiter=',', fmt='%s')\n",
    "        truncatednormalmarginalcoveragefilevague.close()\n",
    "        \n",
    "    with open(\"sampleddatavague\" + str(i) + \".txt\", \"a\") as sampleddatafilevague:\n",
    "        wr = csv.writer(sampleddatafilevague)\n",
    "        wr.writerow(np.array([pointsall[0:i], sampledatavector[times, :]]))\n",
    "        # np.savetxt(sampleddatafile,ci_95_marginal, delimiter=',', fmt='%s')\n",
    "        sampleddatafilevague.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aef81c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates the coverage and average width of the 95% marginal posterior HDI over the 300 runs.\n",
    "\n",
    "order=[2, 14, 4, 32, 59, 67, 48, 0, 9, 36, 18, 7, 15, 19, 27, 33, 6, 5, 57, 12]\n",
    "\n",
    "#For weakly informative prior\n",
    "coveragedata = pd.read_csv(\"checkcoveragemarginalweaklyinformativeprior\"+str(i)+\".txt\",names=['Flownumber','Lower','Higher','True value','Coverage','mean']) \n",
    "\n",
    "#For uninformative prior\n",
    "#coveragedata = pd.read_csv(\"checkcoveragemarginaluninformativeprior\" + str(i) +\".txt\",names=['Flownumber','Lower','Higher','True value','Coverage','mean'])\n",
    "\n",
    "\n",
    "experiments=len(coveragedata.index)/20\n",
    "\n",
    "\n",
    "coveragedatasmaller=coveragedata[['Flownumber', 'Coverage']]\n",
    "\n",
    "calculatecoverage=coveragedata.groupby(['Flownumber']).sum()\n",
    "calculatecoverage['Coveragepercent']=calculatecoverage['Coverage']/experiments\n",
    "\n",
    "print('coverage percent for each variable')\n",
    "print(calculatecoverage.loc[order]['Coveragepercent'])\n",
    "\n",
    "calculatecoverage['averagehdilength']=(calculatecoverage['Higher']-calculatecoverage['Lower'])/experiments    \n",
    "\n",
    "print('average HDI width for each variable')\n",
    "print(calculatecoverage.loc[order]['averagehdilength'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab54815",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7531d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
